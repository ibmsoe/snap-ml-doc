
.. _manual:

Manual
==================================

The Snap Machine Learning (Snap ML) Library is designed to offer fast training of generalized linear models.
It currently supports the following machine learning models:

* :ref:`linear-regression`
* :ref:`svm`
* :ref:`logistic-regression`


.. _linear-regression:

---------------------------
Linear Regression
---------------------------
.. currentmodule:: snap_ml.LinearRegression

:class:`LinearRegression` fits a linear model with coefficients :math:`{\bf w} = (w_1, ..., w_d)` to minimize the residual sum
of squares between the predicted responses :math:`\bf \hat{y}` and the true labels :math:`\bf y` of the training data. 

In order to prevent the model from overfitting ridge regression imposes an 
:math:`L_2` norm penalty on the size of the coefficients. Mathematically Ridge Regression
solves the following optimization problem:

.. math:: \underset{{\bf w}}{\min\,} \frac 1 {2}|| X^\top {\bf w} - {\bf y}||_2^2 + \frac \lambda 2 \|{\bf w}\|_2^2

where :math:`X=[{\bf x}_1,...,{\bf x}_n]` is the training data matrix with samples :math:`\{{\bf x}_i\}_{i\in [n]}` in its columns and :math:`y_i` are the corresponding labels. The regularization strength is controlled by the regularization parameter :math:`\lambda\geq 0`; 
the larger :math:`\lambda` the more robust the model becomes to overfitting.


.. note::
	In order to find an appropriate regularization parameter we recommend to perform cross validation.

*Snap ML* implements different variants of stochastic coordinate descent `[SCD] <References_>`_ and stochastic dual coordinate ascent `[SDCA] <References_>`_ as an algorithm to fit the model parameters :math:`\bf w`. 
In order to optimally support GPUs for training *Snap ML* implements a parallel asynchronous version of these solvers especially designed to leverage the massive parallelism of modern GPUs `[TPASCD17] <References_>`_. 

To train the :class:`LinearRegression` model the ``fit`` method is used; it takes the training data and the labels :math:`X,\bf y` as input and stores the learnt coefficients of the model :math:`\bf w` in its  ``coef_`` member function. The trained model can then be used to make predictions by calling the ``predict`` method on unlabeled data. 

.. code-block:: python

    >>> from snap_ml import LinearRegression
    >>> import numpy as np

    >>> reg = LinearRegression(max_iter = 100, regularizer= 0.1)
    >>> X_train = np.array([[0, 0], [1, 1], [2, 2]])
    >>> y_train = np.array([0, 1, 2])
    >>> reg.fit (X_train, y_train)
    >>> reg.coef_
    [ 0.495,  0.495]
    >>> X_test = np.array([[3, 3], [0, 1]])
    >>> reg.predict(X_test)
    [2.97, 0.495]

	
For more details about the API we refer to the :ref:`python-api-documentation`. 
 

.. _svm:

--------------------------
Support Vector Machine
--------------------------

.. currentmodule:: snap_ml.SupportVectorMachine

Support Vector Machine (SVM) is a supervised learning method which can be applied for regression as well as classification.
Currently *Snap ML* implements :class:`SupportVectorMachine` (SVMs) with a linear kernel function and offers 
:math:`L_2` regularization to prevent the model from overfitting.

Mathematically it solves the following optimization problem:

.. math:: \underset{{\bf w}}{\min\,}  \sum_i [{\bf x}_i^\top {\bf w}  - y_i]_+ +\frac \lambda 2 \|{\bf w}\|_2^2

where :math:`[u]_+=\max(u,0)` denotes the hinge loss with :math:`\{x_i\}_{i\in [n]}` being the training samples and :math:`y_i \in \{\pm 1\}` the corresponding labels. The regularization strength :math:`\lambda>0`  can be controlled by the user through the ``regularizer`` parameter.
The larger :math:`\lambda` the more robust the model becomes to overfitting.
 
*Snap ML* implements stochastic dual coordinate `[SDCA] <References_>`_ and the GPU optimized `[TPASCD17] <References_>`_ as an algorithm to train the SVM classifier. 
SDCA runs on the equivalent SVM dual problem formulation: 

.. math:: \underset{\boldsymbol \alpha}{\min\,}  \sum_i -\alpha_i y_i +\frac 1 {2\lambda} \|X \boldsymbol \alpha\|_2^2

with the constraint :math:`\alpha_i y_i\in[0,1]`.

To train the model the ``fit`` method is used; it takes the training data and the labels :math:`X,{\bf y}` as input and stores the learnt coefficients of the model :math:`{\bf w}` in its  ``coef_`` member function. The trained model can then be used to make predictions by calling the ``predict`` method on unlabeled data. 

.. code-block:: python

    >>> from snap_ml import SupportVectorMachine
    >>> import numpy as np

    >>> reg = SupportVectorMachine(max_iter = 100, regularizer= 1.0)
    >>> X_train = np.array([[0, 0], [1, 1], [2, 2]])
    >>> y_train = np.array([-1,-1,1])
    >>> reg.fit (X_train, y_train)
    >>> reg.coef_
    [ 0.25,  0.25]
    >>> X_test = np.array([[3, 3], [0, 1]])
    >>> reg.predict(X_test)
    [1,-1]


A full example of training a :class:`SupportVectorMachine` model in a real application can be found under :ref:`examples` and for more details about the API we refer to the :ref:`python-api-documentation`. 

    

.. _logistic-regression:

---------------------------
Logistic Regression
---------------------------

.. currentmodule:: snap_ml.LogisticRegression

:class:`LogisticRegression` is a linear model for classification. A logistic model is used to estimate the probability of an outcome based on the features of the input data. In order to prevent the model from overfitting :math:`L_2` regularization is implemented. 

Mathematically Logistic Regression solves the following optimization problem composing of the logistic loss and an :math:`L_2` regularizer:

.. math:: \underset{{\bf w}}{\min\,} \sum_i \log(1+\exp(-y_i {\bf x}_i^\top {\bf w})) + \frac \lambda 2 \|{\bf w}\|_2^2

where :math:`X=[{\bf x}_1,...,{\bf x}_n]` is the training data matrix with samples :math:`\{{\bf x}_i\}_{i\in [n]}` in its columns and :math:`y_i \in \{\pm 1\}` denote the corresponding labels. 
The regularization strength is controlled by the regularization parameter :math:`\lambda\geq 0`; the larger :math:`\lambda` the more robust the model becomes to overfitting. :math:`\lambda` can be specified by the user through the ``regularizer`` input parameter.

*Snap ML* implements stochastic coordinate descent `[SCD] <References_>`_ and stochastic dual coordinate ascent `[SDCA] <References_>`_ as an algorithm to fit the model parameters :math:`{\bf w}`. In order to support GPU acceleration *Snap ML* implements a parallel asynchronous version of these solvers especially designed to leverage the massive parallelism of moderne GPUs `[TPASCD17] <References_>`_. 

The model can be trained using the ``fit`` method which takes the training data and the labels :math:`X,\bf y` as input and stores the coefficients of the learnt model :math:`{\bf w}` in its  ``coef_`` attribute. This model can then be used to make predictions by calling the ``predict`` method on unlabeled data. 

.. code-block:: python

    >>> from snap_ml import LogisticRegression
    >>> import numpy as np

    >>> lr = LogisticRegression(max_iter = 100, regularizer= 0.01)
    >>> X_train = np.array([[0, 0], [1, 1], [2, 2]])
    >>> y_train = np.array([-1,-1,1])
    >>> lr.fit (X_train, y_train)
    >>> lr.coef_
    [0.145, 0.145]
    >>> X_test = np.array([[3,3],[-2,1]])
    >>> lr.predict(X_test)
    [1,-1]
    >>> lr.predict_proba(X_test)
    [[0.295,0.705]
     [0.536, 0.464]]


A full example of training a :class:`LogisticRegression` model in Snap ML can be found here :ref:`examples` and for more details about the API we refer to the :ref:`python-api-documentation`. 

=======================
References
=======================

:[SCD]: *Y. Nesterov*. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 2012.

:[SDCA]: *Shai Shalev-Shwartz and Tong Zhang*. Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization. Journal of Machine Learning Research, 2013.

:[TPASCD17]: *Thomas Parnell, Celestine DÃ¼nner, Kubilay Atasu, Manolis Sifalakis, and Haris Pozidis*. Large-Scale Stochastic Learning using GPUs. IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), 2017.


